{"1": "Fig. 1  Schematic diagram of underwater imaging the image intensity. In an underwater environment, light en- counters suspended particles within the water medium before reaching the camera. This can alter the direction of the light from the camera, leading to image blurring, resulting in low contrast and a fog-like effect [1]. Then, underwater images suffer from noise and distortion caused by water turbidity and low light conditions. Due to the impact of light scattering, absorption, and noise, underwater images have reduced con- trast, color distortion, and blurred details. Fig. 1 illustrates the schematic representation of the underwater imaging. In accordance with the Jaffe-McGlamery model [2], underwater images can be modeled as where  U T  represents the final underwater image captured by the camera,  U d  is the direct attenuation (light reflected by the object without scattering), and  U fs  and  U bs  correspond to the forward and backward scattering components, respec- tively [2]. 2. EXISTING RESEARCH Various approaches are utilized to enhance the visual clar- ity of underwater images, and they can be classified into three main groups: non-physical model-based methods, physical model-based methods, and data-driven methods. Non-physical model-based methods such as histogram equal- ization and filtering methods enhance the visual effect by", "2": "Fig. 2  Architectures of (a) conventional Shallow-UWnet and (b) proposed method L MSE  =   1 n n X i =1 ( I GT  \u2212 I EN ) 2 (2) n n X i =1 \u2225 \u03d5 ( I GT  ) i  \u2212 \u03d5 ( I EN ) i \u2225 2 (3) L total  =  L MSE  +  L V GG (4)", "3": "Fig. 3  Power spectrum sparsity of image other neurons within a single channel. Then,  i  denotes the spatial dimension,  N  represents the total number of neurons, and  \u03b1  is a hyper-parameter coefficient set to 0.0001 [16]. Fi- nally, the SimAM can be compromised as e Y  = sigmoid \u0012  1 where the total energy function of  \u03f5 T  is denoted as  E  and   J indicates dot product operation and   e Y  and  Y  represent the refined and original features, respectively [16]. 4. EXPERIMENTAL RESULTS 4.1. Implementation details The proposed model is trained using the ADAM optimizer with a learning rate of 0.0002 and layer dropout of 0.2, a batch size of 1, and 50 epochs, which is conducted in the same con- figuration as Shallow-UWnet. The input images are resized to 256 \u00d7 256. The model employs the PyTorch framework and is trained on an Intel Core i9 CPU, Nvidia GeForce RTX 4070, and 32GB of RAM. 4.2. Datasets Currently, there are available paired real-world underwater image datasets [17][18] like EUVP [6], UFO-120 [8], and UIEB [5]. The images in the EUVP [6] dataset were taken with seven distinct cameras during deep-sea exploration and human-robot studies [19]. In this experiment, we utilized 3500 pairs of images from EUVP-ImageNet [6] for the train- ing, while the remaining 200 image pairs were used for val- idation. The testing datasets are as follows. EUVP-Dark [6] dataset includes 5,500 paired images that capture dark-hazed underwater scenes. For testing, 1000 images were used in ac- cordance with Shallow-UWnet model. UFO-120 dataset cap- tures high-quality images during oceanic explorations. Dis- torted images in UFO-120 dataset were created using style transfer, and 120 paired images were utilized as a benchmark", "4": "Fig. 4  Comparison of different methods on the EUVP-Dark, UFO-120, and UIEB datasets [from top to bottom] Raw Input Image, WaterNet, FUnIE-GAN, Shallow-UWnet, Proposed method (SLPF), and Ground Truth and UGAN [7] on the EUVP-Dark and UFO-120 datasets. The two-step pipeline model, iDehaze [9], achieves superior UIQM scores, but it has the lowest PSNR results in compari- son with others. Finally, although the proposed method does not perform superior to others, it achieves comparable results across all three datasets, as shown in Table 1. As shown in Table 2, the proposed method has fewer trainable parame- ters when evaluating the model\u2019s performance than others. Consequently, the proposed method with a smaller number of trainable parameters is suitable for underwater AUV and ROV devices with limited computing power. Other SOTA methods, such as iDehaze [9] and MDCNN-VGG [10], do not highlight their efficiency in terms of being lightweight. Undoubtedly, the deeper architectures in iDehaze and MDCNN-VGG re- sult in a larger number of parameters compared with others. Regarding testing time, Xing et al. [12] outperforms other methods. Nevertheless, the proposed method, utilizing the impulse response of SLPF, is also suitable for real-time un- derwater image enhancement. In Fig. 4, a visual comparison of the proposed method with the impulse response of SLPF, WaterNet [5], FUnIE- GAN [6], and Shallow-UWnet [11] is shown where in each image the resulting PSNR is also shown. In each dataset, the raw input images and reference images for ground truth are provided for visual comparison. We have the following ob- servations. (1) EUVP-Dark Dataset : WaterNet has artificial colors and noise artifacts in both blue and green-hued images com- pared with the proposed method. In comparison with the FUnIE-GAN, there are incorrect color corrections, especially noticeable in the fin of the fish image. Conversely, Shallow- UWnet and the proposed method have a noticeable effect on Authorized licensed use limited to: IEEE Xplore. Downloaded on October 29,2024 at 18:03:33 UTC from IEEE Xplore.  Restrictions apply. "}